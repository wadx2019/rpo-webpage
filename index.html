<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>RPO</title>
    <link rel="icon" href="favicon-32x32.png" title="RPO">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="css/normalize.css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/cayman.css">
  </head>
  <body>
    <section class="page-header">
      <h1 class=>NeurIPS 2023 Poster</h1>
      <h1 class="project-name">Reduced Policy Optimization for Continuous Control with Hard Constraints</h1>

      <h2 class="project-tagline">Shutong Ding<sup>1</sup> &emsp; Jingya Wang<sup>1</sup> &emsp; Yali Du<sup>2</sup> &emsp; Ye Shi<sup>1</sup>* &emsp;</h2>
      <h2 class="project-tagline"><sup>1</sup>ShanghaiTech University &emsp; <sup>2</sup>Kingâ€™s College London</h2>

      <a href="https://openreview.net/forum?id=fKVEMNmWqU" class="btn">paper</a>
      <a href="https://github.com/wadx2019/rpo" class="btn">code</a>
      <a href="" class="btn">slides</a>
      <a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202023/70911.png" class="btn">poster</a>
      <!-- <br> -->
      <a href="https://recorder-v3.slideslive.com/?share=89819&s=cbd1f0a9-6388-43dd-922c-080d55f438d0" class="btn">video (English)</a>
      <!-- <a href="" class="btn">video (Chinese)</a>
      <a href="" class="btn">blog (Chinese)</a> -->
    </section>

    <section class="main-content">
      <p>Recent advances in constrained reinforcement learning (RL) have endowed reinforcement learning with certain safety guarantees. However, deploying existing constrained RL algorithms in continuous control tasks with general hard constraints remains challenging, particularly in those situations with non-convex hard constraints. Inspired by the generalized reduced gradient (GRG) algorithm, a classical constrained optimization technique, we propose a reduced policy optimization (RPO) algorithm that combines RL with GRG to address general hard constraints. RPO partitions actions into basic actions and nonbasic actions following the GRG method and outputs the basic actions via a policy network. Subsequently, RPO calculates the nonbasic actions by solving equations based on equality constraints using the obtained basic actions. The policy network is then updated by implicitly differentiating nonbasic actions with respect to basic actions. Additionally, we introduce an action projection procedure based on the reduced gradient and apply a modified Lagrangian relaxation technique to ensure inequality constraints are satisfied. To the best of our knowledge, RPO is the first attempt that introduces GRG to RL as a way of efficiently handling both equality and inequality hard constraints. It is worth noting that there is currently a lack of RL environments with complex hard constraints, which motivates us to develop three new benchmarks: two robotics manipulation tasks and a smart grid operation control task. With these benchmarks, RPO achieves better performance than previous constrained RL algorithms in terms of both cumulative reward and constraint violation. We believe RPO, along with the new benchmarks, will open up new opportunities for applying RL to real-world problems with complex constraints.
      <p><img src="rpo-pipline.png" alt="" style="max-width:100%;"></p>

      <h1>
      <!-- <a id="user-content-header-1" class="anchor" href="#header-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Video</h1> -->
        
      <!-- <div class="video"><iframe class="video" frameborder="0" width="100%" height="500"
        src="">
      </iframe></div> -->

      <h1>
      <a id="user-content-header-1" class="anchor" href="#header-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cite our work</h1>
      If you find this work useful in your research, please consider citing:
      <p><div class="highlight highlight-Javascript"><pre>@inproceedings{
ding2023reduced,
title={Reduced Policy Optimization for Continuous Control with Hard Constraints},
author={Shutong Ding and Jingya Wang and Yali Du and Ye Shi},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=fKVEMNmWqU}
}</pre></div></p>

      <footer class="site-footer">
        <!-- <span class="site-footer-owner"><a href="https://github.com/jasonlong/cayman-theme">Cayman</a> is maintained by <a href="https://github.com/jasonlong">jasonlong</a>.</span> -->
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>

    </section>

  </body>
</html>
